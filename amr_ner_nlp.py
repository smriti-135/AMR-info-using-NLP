# -*- coding: utf-8 -*-
"""AMR NER NLP

Automatically generated by Colab Pro.

Original file is located at
    https://colab.research.google.com/drive/1BX9LmBRaG42ngsci6g2f0SBw_33jA1us
"""

import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import torch
import time
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix, classification_report, f1_score
from sklearn.preprocessing import MultiLabelBinarizer
import torch.optim as optim
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
import json
from spacy.training import offsets_to_biluo_tags
from spacy.lang.en import English
from spacy.tokenizer import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from pprint import pprint

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_json("/content/drive/MyDrive/project_9_dataset.jsonl", lines=True)
labels_data = pd.read_json("/content/drive/MyDrive/project_9_labels.json")

data

labels_data

## code for making lists of datas and storing info
text=[]
label=[]
for i in range(len(data)):
  ## checking if annotation are there or not
  if data['annotations'][i]!=[]:
    ## checking how many annotations are there, if more than one, enter in loop
    if len(data['annotations'][i])>1:
      text.append(data['text'][i])
      start_3=[]
      end_3=[]
      label_3=[]
      for j in range(len(data['annotations'][i])):
        try:
          ##taking the start_offset and end_offset and slicing the word from text and fixing it the bug fix is upto the next "pass".
          start=data['annotations'][i][j]['start_offset']
          end=data['annotations'][i][j]['end_offset']
          var=text[i][start:end]
          bef=len(var)
          var_one=var.strip()
          aft=len(var_one)
          if bef!=aft:
            if var[-1]==" ":
              end-=1
            else:
              start+=1
          end_new=end
          start_new=start
        except:
          pass
        ## appending the FIXED start_offset and end_offset ints to a list
        start_3.append(start_new)
        end_3.append(end_new)
        ## mapping the label number to the real label itself, this code does that
        dat=data['annotations'][i][j]['label']
        for k in range(len(labels_data)):
          if labels_data['id'][k]==dat:
            dat=labels_data['text'][k]
        label_3.append(dat)
        result=list(zip(start_3, end_3, label_3))
      label.append(result)
      ## for sentences with exactly 1 annotation, no for loop needed and one time thing.
    elif len(data['annotations'][i])==1:
      text.append(data['text'][i])
      one=[]
      two=[]
      three=[]
      ## same logic for fixing bug code
      try:
        start=data['annotations'][i][0]['start_offset']
        end=data['annotations'][i][0]['end_offset']
        var=text[i][start:end]
        bef=len(var)
        var_one=var.strip()
        aft=len(var_one)
        if bef!=aft:
          if var[-1]==" ":
            end-=1
          else:
            start+=1
        end_new=end
        start_new=start
      except:
        pass
      one.append(start_new)
      two.append(end_new)
      ## swapping label number with label code
      dat=data['annotations'][i][0]['label']
      for l in range(len(labels_data)):
        if labels_data['id'][l]==dat:
          dat=labels_data['text'][l]
      three.append(dat)
      result=list(zip(one, two, three))
      label.append(result)

  else:
    ## code for adding lines with no annotations
    text.append(data['text'][i])
    one=[]
    two=[]
    three=[]
    one.append("")
    two.append("")
    three.append("")
    result=list(zip(one, two, three))
    label.append(result)

for i in range(len(data)):
  print(f"{i} Text is : {text[i]}, label is: {label[i]} ")

## code for making lists of datas and storing info
##----------------------------- GENERAL CODE FOR OTHER DATASETS TOO--------------------------------
# text=[]
# label=[]

# for i in range(len(data)):
#   if data['annotations'][i]!=[]:
#     if len(data['annotations'][i])>1:
#       text.append(data['text'][i])
#       start_3=[]
#       end_3=[]
#       label_3=[]
#       for j in range(len(data['annotations'][i])):
#         try:
#           start=data['annotations'][i][j]['start_offset']
#           end=data['annotations'][i][j]['end_offset']
#           var=text[i][start:end]
#           bef=len(var)
#           var_one=var.strip()
#           aft=len(var_one)
#           if bef!=aft:
#             if var[-1]==" ":
#               end-=1
#             else:
#               start+=1
#           end_new=end
#           start_new=start
#         except:
#           pass
#         start_3.append(start_new)
#         end_3.append(end_new)
#         dat=data['annotations'][i][j]['label']
#         for k in range(len(labels_data)):
#           if labels_data['id'][k]==dat:
#             dat=labels_data['text'][k]
#         label_3.append(dat)
#         result=list(zip(start_3, end_3, label_3))
#       label.append(result)
#     elif len(data['annotations'][i])==1:
#       text.append(data['text'][i])
#       one=[]
#       two=[]
#       three=[]
#       try:
#         start=data['annotations'][i][0]['start_offset']
#         end=data['annotations'][i][0]['end_offset']
#         var=text[i][start:end]
#         bef=len(var)
#         var_one=var.strip()
#         aft=len(var_one)
#         if bef!=aft:
#           if var[-1]==" ":
#             end-=1
#           else:
#             start+=1
#         end_new=end
#         start_new=start
#       except:
#         pass
#       one.append(start_new)
#       two.append(end_new)
#       dat=data['annotations'][i][0]['label']
#       for l in range(len(labels_data)):
#         if labels_data['id'][l]==dat:
#           dat=labels_data['text'][l]
#       three.append(dat)
#       result=list(zip(one, two, three))
#       label.append(result)

#   else:
#     text.append(data['text'][i])
#     one=[]
#     two=[]
#     three=[]
#     one.append("")
#     two.append("")
#     three.append("")
#     result=list(zip(one, two, three))
#     label.append(result)

# labels_data = pd.read_json("project_9_labels.json")
# dat=36
# for i in range(len(labels_data)):
#   if labels_data['id'][i]==dat:
#     dat=labels_data['text'][i]
# dat
# #label swapper code
# for i in range(len(label)):
#   for j in range(len(labels_data)):
#     if label[i]==labels_data['id'][j]:
#       label[i]=labels_data['text'][j]

##### bug fix code for space errors
# for i in range(len(text)):
#   try:
#     start_new = int(start[i])
#     end_new = int(end[i])
#     var=text[i][start_new:end_new]
#     bef=len(var)
#     var_one=var.strip()
#     aft=len(var_one)
#     if bef!=aft:
#       if var[-1]==" ":
#         end[i]-=1
#       else:
#         start[i]+=1
#     end_new=end[i]
#     start_new=start[i]
#   except:
#     pass

#code for printing tagged sentences
list_of_lines=[]
list_of_tags=[]
blank_list_of_lines=[]
for i in range(len(data)):
  nlp=English()
  offsets = label[i]
  doc = nlp(text[i])
  try:
    tags = offsets_to_biluo_tags(nlp.make_doc(text[i]), offsets)
    tags_new = tags
    list_of_lines.append([token.text for token in doc])
    list_of_lines.append('?')
    list_of_tags.append(tags_new)
    list_of_tags.append('?')
    print([token.text for token in doc])
    print(tags_new)
  except:
    text_new=text[i]
    blank_list_of_lines.append(text_new)

len(blank_list_of_lines), len(list_of_lines), len(list_of_tags)

# new=[]
# for i in range(len(total)):
#   new.append(len(total[i]))
# max(new)

total = blank_list_of_lines + list_of_lines
len(max(total, key=len))

words=[]
from spacy.lang.en import English
for i in tqdm(range(len(blank_list_of_lines))):
  nlp = English()
  doc = nlp(blank_list_of_lines[i])
  for token in doc:
    token=str(token)
    words.append(token)
  words.append('?')

tags_n=[]
for i in range(len(words)):
  if words[i] != ' ':
    tags_n.append('O')
  else:
    tags_n.append(" ")

## code to list all words and tags
list_of_words=[]
list_of_tags_new=[]
for i in range(len(list_of_lines)):
  for word in list_of_lines[i]:
    list_of_words.append(word)
for i in range(len(list_of_tags)):
  for tag in list_of_tags[i]:
    list_of_tags_new.append(tag)
final_words = list_of_words + words
final_tags = list_of_tags_new + tags_n

final_tags, final_words

len(final_words), len(final_tags)

## code to convert the lists into a dataframe and ultimately into a csv
dataframe = pd.DataFrame(list(zip(final_words,final_tags)), columns=['Words', 'Tags'])
dataframe.to_csv('Data_two.csv', index=False)

# data_one = pd.read_csv("Data_one.csv")
# data_two = pd.read_csv("Data_two.csv")
# # data_two = pd.read_csv("Data_one_new.csv")

# word = data_two['Words']
# sen=1
# Sentence=[]
# for i in range (len(word)):
#   Sentence.append(sen)
#   if word[i] =='?':
#     sen+=1
# data_two['Sentence#'] = Sentence

# for i in range (len(data_two['Words'])):
#   if data_two['Words'][i]=='?':
#     data_two['Sentence#'][i]='?'
#     data_two['Tags'][i]='?'

# for i in range(len(data_two['Tags'])):
#   if data_two['Tags'][i]=='-':
#     data_two['Tags'][i]='O'

# data_two.to_csv('Data_one_new.csv', index=False)

# data_one=pd.read_csv("Data_one_new.csv")
# data_two=pd.read_csv("Data_two_new.csv")
# data_one

# data_one_words=list(data_one['Words'])
# data_one_tags=list(data_one['Tags'])
# data_one_Sen=list(data_one['Sentence#'])
# data_two_words=list(data_two['Words'])
# data_two_tags=list(data_two['Tags'])
# data_two_Sen=list(data_two['Sentence#'])

# len(data_one_words), len(data_two_words)

# words = data_one_words + data_two_words
# tags = data_one_tags + data_two_tags
# sen = data_one_Sen + data_two_Sen
# len(words) , len(tags), len(sen)

# dataframe = pd.DataFrame(list(zip(words, tags)), columns=['Words', 'Tags'])
# dataframe

# word = dataframe['Words']
# sen=1
# Sentence=[]
# for i in range (len(word)):
#   Sentence.append(sen)
#   if word[i] =='?':
#     sen+=1
# dataframe['Sentence#'] = Sentence

# for i in range(len(dataframe['Sentence#'])):
#   if dataframe['Words'][i]=='?':
#     dataframe['Sentence#'][i]='?'

# dataframe.to_csv("Data_final.csv", index=False)

"""Data preprocessing"""

import pandas as pd
data = pd.read_csv("Data_final.csv")

count=0
for i in range(len(data['Words'])):
  if data['Words'][i]=='nan':
    count+=1
count

data

count=0
for i in range(len(data['Tags'])):
  if data['Tags'][i]==' ':
    count+=1
count

listoflistofwords = data.groupby('Sentence#') ['Words'].apply(lambda x: x.values.tolist()).tolist()
listoflistoftags = data.groupby('Sentence#') ['Tags'].apply(lambda x: x.values.tolist()).tolist()

len(listoflistofwords)

count=0
for i in range(len(data['Tags'])):
    if data['Tags'][i]==' ':
      count+=1
count

listoflines=[]
for i in range(len(listoflistofwords)):
  listoflines.append(" ".join(listoflistofwords[i]))
listoflines[679]

length=[]
for i in range(len(listoflines)):
  length.append(len(listoflines[i]))

listoflinesnew=listoflines[:-1]
listoftagsnew=listoflistoftags[:-1]

! pip install -U accelerate
! pip install -U transformers

import transformers
print(transformers.__version__)

# Load model directly
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-v1.1")
model = AutoModel.from_pretrained("dmis-lab/biobert-v1.1")

ex_sent = ("Nimisha is my best friend")
ex_sent_tokens = tokenizer.tokenize(ex_sent)
print(ex_sent_tokens)

#tokenizing
temp=[]
new=[]
for i in range(len(listoflinesnew)):
  var = listoflinesnew[i]
  var=var.split()
  for i in range(len(var)):
    temp.append(tokenizer.tokenize(var[i]))
  new.append(temp)
  temp=[]

len(new)

#aligning labels with splitted words
new_tags = []
temp=[]
for i in range(len(new)):
  for j in range(len(new[i])):
    if len(new[i][j]) == 1:
      temp.append(listoftagsnew[i][j])
    else:
      for k in range(len(new[i][j])):
        if k == 0:
          temp.append(listoftagsnew[i][j])
        else:
          temp.append('X')
  new_tags.append(temp)
  temp=[]

# tokening
tokenized=[]
for i in range(len(listoflinesnew)):
  val = tokenizer.tokenize(listoflinesnew[i])
  tokenized.append(val)

# converting tokens to input ids
tokenized_ids=[]
for i in range(len(tokenized)):
  tokenized_ids.append(tokenizer.convert_tokens_to_ids(tokenized[i]))

# appending to a list all token ids
length=[]
for i in range(len(tokenized_ids)):
  length.append(len(tokenized_ids[i]))
max_length = 318

# padding token ids
from tensorflow.keras.preprocessing.sequence import pad_sequences
tokenized_ids_padded=[]
zero=[]
for sen in tokenized_ids:
  pads = pad_sequences([sen], maxlen=max_length, padding='post')
  tokenized_ids_padded.append(pads)

tokenized_ids_padded[0]

# converting padded ids to list regular
final=[]
for i in range(len(tokenized_ids_padded)):
  finals = tokenized_ids_padded[i].tolist()
  final.append(finals)

# processing
padded_ids=[]
temp=[]
for i in range(len(final)):
  for j in range(len(final[i])):
    temp.append(final[i][j])
  padded_ids.append(temp[0])
  temp=[]

print(padded_ids[0])
print(tokenizer.convert_ids_to_tokens(padded_ids[0]))
print(new_tags[0])
print(attention_mask[0])
print(len(padded_ids[0]))
print(len(new_tags[0]))
print(len(attention_mask[0]))

#attention mask
attention_mask=[]
temp=[]
for i in range(len(padded_ids)):
  for j in range(len(padded_ids[i])):
    if padded_ids[i][j] != 0:
      temp.append(1)
    elif padded_ids[i][j] == 0:
      temp.append(0)
  attention_mask.append(temp)
  temp=[]

attention_mask[0]

final_tags=[]
temp=[]
for i in range(len(new_tags)):
  for j in range(len(new_tags[i])):
    if new_tags[i][j]==' ':
      temp.append('O')
    else:
      temp.append(new_tags[i][j])
  final_tags.append(temp)
  temp=[]

lists=[]
count=0
for i in range(len(final_tags)):
  for j in range(len(final_tags[i])):
    if final_tags[i][j] in lists:
      pass
    else:
      lists.append(final_tags[i][j])

lists.insert(0,'NULL')
lists

tag2index = {tag : idx for idx, tag in enumerate(lists, 1)}

tag2index

tags_ids=[]
temp=[]
for i in range(len(final_tags)):
  for j in range(len(final_tags[i])):
    temp.append(tag2index[final_tags[i][j]])
  tags_ids.append(temp)
  temp=[]

padded_tags=[]
max_length=318
for sen in tags_ids:
  pads = pad_sequences([sen], maxlen=max_length, padding='post')
  padded_tags.append(pads)
final=[]
for i in range(len(padded_tags)):
  finals = padded_tags[i].tolist()
  final.append(finals)
padded_tags_final=[]
temp=[]
for i in range(len(final)):
  for j in range(len(final[i])):
    temp.append(final[i][j])
  padded_tags_final.append(temp[0])
  temp=[]

print(padded_ids[0])
print(padded_tags_final[0])
print(attention_mask[0])

from sklearn.model_selection import train_test_split
token_train, token_test, tags_train, tags_test = train_test_split(padded_ids, padded_tags_final, test_size=0.3, shuffle=True)
mask_train, mask_test = train_test_split(attention_mask, test_size=0.3, shuffle=True)

if torch.cuda.is_available():
 dev = "cuda"
else:
 dev = "cpu"
device = torch.device(dev)

token_train_tensor = torch.tensor(token_train, device=device)
token_test_tensor = torch.tensor(token_test, device=device)
tags_train_tensor = torch.tensor(tags_train, device=device)
tags_test_tensor = torch.tensor(tags_test, device=device)
mask_train_tensor = torch.tensor(mask_train, device=device)
mask_test_tensor = torch.tensor(mask_test, device=device)

token_test_tensor[0], tags_test_tensor[0], mask_test_tensor[0]

print(token_train_tensor.shape)
print(token_test_tensor.shape)
print(tags_train_tensor.shape)
print(tags_test_tensor.shape)
print(mask_train_tensor.shape)
print(mask_test_tensor.shape)

Training_set = TensorDataset(token_train_tensor, tags_train_tensor, mask_train_tensor)
train_loader = DataLoader(Training_set, batch_size=32)

Testing_set = TensorDataset(token_test_tensor, tags_test_tensor, mask_test_tensor)
test_loader = DataLoader(Testing_set, batch_size=32)

if torch.cuda.is_available():
  dev = "cuda:0"
else:
  dev = "cpu"
device = torch.device(dev)
print(device)

class mymodel(nn.Module):
  def __init__(self):
    super(mymodel, self).__init__()
    self.bert_layer = AutoModel.from_pretrained("dmis-lab/biobert-v1.1")
    self.dropout = nn.Dropout(0.5)
    self.output = nn.Linear(self.bert_layer.config.hidden_size, len(tag2index))

  def forward(self, inputs, attention_mask):
    bert_outs = self.dropout(self.bert_layer(inputs, attention_mask=attention_mask)[0])
    logits = torch.argmax(self.output(bert_outs), dim=2)
    return logits

model = mymodel()
model.to(device)

loss_val=[]
epochs=10
optimizer = optim.Adam(params=model.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss()
model.train()
for i in range(epochs):
  running_loss=0
  for data in tqdm(train_loader):
    batch = tuple(t.to(device) for t in data)
    input_id, input_tag, input_mask = batch
    output = model(input_id, input_mask)
    loss = loss_fn(output.float(), input_tag.float())
    # print(output.shape)
    # print(input_tag.shape)
    loss.requires_grad=True
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    loss_val.append(loss.item())
    running_loss += loss.item()
    #print(loss.item())

plt.plot(loss_val)

model.eval()
pred=[]
with torch.no_grad():
  correct = 0
  total = 0
  for data in tqdm(test_loader):
    batch = tuple(t.to(device) for t in data)
    input_id, input_tag, input_mask = batch
    output = model(input_id, input_mask)
    temp=output.tolist()
    pred.append(temp)

len(pred)

exp = tags_test_tensor.tolist()
len(exp)

tempo=[]
for i in range(len(exp)):
  for j in range(len(exp[i])):
    if exp[i][j] in tempo:
      pass
    else:
      tempo.append(exp[i][j])

predicted=[]
for i in range(len(pred)):
  for j in range(len(pred[i])):
    predicted.append(pred[i][j])

len(predicted)

m = MultiLabelBinarizer().fit(exp)

print(classification_report(m.transform(exp), m.transform(predicted), target_names=lists, zero_division='warn'))

print(f1_score(m.transform(exp), m.transform(predicted), average='micro', zero_division='warn'))













import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from torchvision import datasets
from torchvision import transforms
from tqdm import tqdm

tensor_transform = transforms.ToTensor()

# Download the MNIST Dataset
dataset = datasets.MNIST(root = "./data",
                         train = True,
                         download = True,
                         transform = tensor_transform)

# DataLoader is used to load the dataset
# for training
loader = torch.utils.data.DataLoader(dataset = dataset,
                                     batch_size = 32,
                                     shuffle = True)
if torch.cuda.is_available():
  dev='cuda:0'
else:
  dev="cpu"

input_size=784
hidden_enc=[128, 64, 36, 18, 9]
hidden_dec=[18, 36, 64, 128, 784]
class autoenc(nn.Module):
    def __init__(self, hidden_enc, hidden_dec, input_size):
        super().__init__()
        self.layers_enc = nn.ModuleList()
        for size in hidden_enc:
            self.layers_enc.append(nn.Linear(input_size, size))
            input_size = size

        self.layers_dec = nn.ModuleList()
        for size in hidden_dec:
            self.layers_dec.append(nn.Linear(input_size, size))
            input_size = size
        self.layers_dec.append(nn.Sigmoid())

    def forward(self, x):
        for layer in self.layers_enc:
            x = F.relu(layer(x))
        counter = 1
        for layer in self.layers_dec:
            x = F.relu(layer(x))
            counter += 1
            if counter == len(self.layers_dec):
                break
        return x

model=autoenc(hidden_enc, hidden_dec, input_size)
model.to(dev)

loss_func = nn.MSELoss()
optimizer=torch.optim.Adam(model.parameters(), lr=1e-1, weight_decay=1e-8)

print(f"The model is using the {dev}")
epochs=20
outputs=[]
loss_list=[]
for epoch in range(epochs):
    for (image, _) in tqdm(loader):
        image=image.reshape(-1, 28*28).to(dev)
        reconstructed = model(image)
        loss = loss_func(reconstructed, image)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        loss_list.append(loss.detach().cpu())
    outputs.append((epochs, image.detach().cpu(), reconstructed.detach().cpu()))

plt.style.use('fivethirtyeight')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.plot(loss_list[-100:])

plt.style.use('fivethirtyeight')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.plot(loss_list[-30000:])

epoch, image, reconstructed = outputs[0]
image= image.detach().cpu()
reconstructed = reconstructed.detach().cpu()
for i, item in enumerate(image):
  item = item.reshape(-1, 28, 28)
  plt.imshow(item[0])

for i, item in enumerate(reconstructed):
  item = item.reshape(-1, 28, 28)
  plt.imshow(item[0])

outputs

output

